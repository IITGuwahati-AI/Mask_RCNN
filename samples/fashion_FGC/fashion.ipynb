{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.8.1-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python38164bittestvenvd6c2fb1a1a394538bb7efa1506920219",
   "display_name": "Python 3.8.1 64-bit ('test': venv)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import json\n",
    "import glob\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "\n",
    "from imgaug import augmenters as iaa\n",
    "from sklearn.model_selection import StratifiedKFold, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(r'c:\\\\Users\\\\HP\\\\Documents\\\\GitHub\\\\Mask_RCNN\\\\fashion_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Using TensorFlow backend.\n"
    }
   ],
   "source": [
    "sys.path.append(r'c:\\\\Users\\\\HP\\\\Documents\\\\GitHub\\\\Mask_RCNN')\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "from mrcnn.model import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "COCO_WEIGHTS_PATH = 'mask_rcnn_coco.h5'\n",
    "NUM_CATS = 46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\nConfigurations:\nBACKBONE                       resnet101\nBACKBONE_STRIDES               [4, 8, 16, 32, 64]\nBATCH_SIZE                     2\nBBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\nCOMPUTE_BACKBONE_SHAPE         None\nDETECTION_MAX_INSTANCES        100\nDETECTION_MIN_CONFIDENCE       0.7\nDETECTION_NMS_THRESHOLD        0.3\nFPN_CLASSIF_FC_LAYERS_SIZE     1024\nGPU_COUNT                      1\nGRADIENT_CLIP_NORM             5.0\nIMAGES_PER_GPU                 2\nIMAGE_CHANNEL_COUNT            3\nIMAGE_MAX_DIM                  1024\nIMAGE_META_SIZE                59\nIMAGE_MIN_DIM                  1024\nIMAGE_MIN_SCALE                0\nIMAGE_RESIZE_MODE              square\nIMAGE_SHAPE                    [1024 1024    3]\nLEARNING_MOMENTUM              0.9\nLEARNING_RATE                  0.001\nLOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0, 'mrcnn_attr_loss': 1.0}\nMASK_POOL_SIZE                 14\nMASK_SHAPE                     [28, 28]\nMAX_GT_INSTANCES               100\nMEAN_PIXEL                     [123.7 116.8 103.9]\nMINI_MASK_SHAPE                (112, 112)\nNAME                           fashion\nNUM_ATTR                       294\nNUM_CLASSES                    47\nPOOL_SIZE                      7\nPOST_NMS_ROIS_INFERENCE        1000\nPOST_NMS_ROIS_TRAINING         2000\nPRE_NMS_LIMIT                  6000\nROI_POSITIVE_RATIO             0.33\nRPN_ANCHOR_RATIOS              [0.5, 1, 2]\nRPN_ANCHOR_SCALES              (16, 32, 64, 128, 256)\nRPN_ANCHOR_STRIDE              1\nRPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\nRPN_NMS_THRESHOLD              0.7\nRPN_TRAIN_ANCHORS_PER_IMAGE    256\nSTEPS_PER_EPOCH                1000\nTOP_DOWN_PYRAMID_SIZE          256\nTRAIN_BN                       False\nTRAIN_ROIS_PER_IMAGE           200\nUSE_MINI_MASK                  True\nUSE_RPN_ROIS                   True\nVALIDATION_STEPS               200\nWEIGHT_DECAY                   0.0001\n\n\n"
    }
   ],
   "source": [
    "class FashionConfig(Config):\n",
    "    NAME = \"fashion\"\n",
    "    NUM_CLASSES = NUM_CATS + 1 # +1 for the background class\n",
    "    \n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 2 # a memory error occurs when IMAGES_PER_GPU is too high\n",
    "    \n",
    "    RPN_ANCHOR_SCALES = (16, 32, 64, 128, 256)\n",
    "    #DETECTION_NMS_THRESHOLD = 0.0\n",
    "    \n",
    "    # STEPS_PER_EPOCH should be the number of instances \n",
    "    # divided by (GPU_COUNT*IMAGES_PER_GPU), and so should VALIDATION_STEPS;\n",
    "    # however, due to the time limit, I set them so that this kernel can be run in 9 hours\n",
    "    STEPS_PER_EPOCH = 1000\n",
    "    VALIDATION_STEPS = 200\n",
    "\n",
    "    ## My changes CA\n",
    "    BACKBONE = 'resnet101'\n",
    "    \n",
    "    IMAGE_MIN_DIM = 1024\n",
    "    IMAGE_MAX_DIM = 1024    \n",
    "    IMAGE_RESIZE_MODE = 'square'\n",
    "\n",
    "    MINI_MASK_SHAPE = (112, 112)  # (height, width) of the mini-mask\n",
    "\n",
    "    NUM_ATTR = 294\n",
    "\n",
    "    LOSS_WEIGHTS = {\n",
    "        \"rpn_class_loss\": 1.,\n",
    "        \"rpn_bbox_loss\": 1.,\n",
    "        \"mrcnn_class_loss\": 1.,\n",
    "        \"mrcnn_bbox_loss\": 1.,\n",
    "        \"mrcnn_mask_loss\": 1.,\n",
    "        \"mrcnn_attr_loss\":1.\n",
    "    }\n",
    "\n",
    "\n",
    "    \n",
    "config = FashionConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_DIR/\"label_descriptions.json\") as f:\n",
    "    label_descriptions = json.load(f)\n",
    "\n",
    "class_names = [x['name'] for x in label_descriptions['categories']]\n",
    "attr_names = [x['name'] for x in label_descriptions['attributes']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "46 294\n"
    }
   ],
   "source": [
    "print(len(class_names),len(attr_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_df = pd.read_csv(DATA_DIR/\"train_small.csv\")\n",
    "segment_df['AttributesIds'] = segment_df['AttributesIds'].apply(lambda x:tuple([int(i) for i in x.split(',')]))\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Total images:  20\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                                                      EncodedPixels  \\\nImageId                                                                               \n00000663ed1ff0c4e0132b9b9ac53f6e  [6068157 7 6073371 20 6078584 34 6083797 48 60...   \n000aac3870ea7c59ca0333ffa5327323  [8971124 17 8975101 49 8979077 82 8983053 116 ...   \n00102686c01fa625aba3b2478f48f415  [1030495 6 1032230 18 1033965 30 1035701 41 10...   \n0017af9f7fa8ce9e7558bb924cec324e  [169688 9 171477 25 173265 43 175053 60 176841...   \n0055347a114b215f8f469fec9e38c272  [893062 1 894561 4 896061 7 897560 10 899059 1...   \n\n                                                            ClassId  \\\nImageId                                                               \n00000663ed1ff0c4e0132b9b9ac53f6e  [6, 0, 28, 31, 32, 32, 31, 29, 4]   \n000aac3870ea7c59ca0333ffa5327323            [10, 33, 3, 31, 31, 33]   \n00102686c01fa625aba3b2478f48f415                   [31, 31, 33, 10]   \n0017af9f7fa8ce9e7558bb924cec324e                 [6, 1, 31, 31, 33]   \n0055347a114b215f8f469fec9e38c272                       [33, 31, 10]   \n\n                                                                      AttributesIds  \\\nImageId                                                                               \n00000663ed1ff0c4e0132b9b9ac53f6e  [(115, 136, 143, 154, 230, 295, 316, 317), (11...   \n000aac3870ea7c59ca0333ffa5327323  [(104, 115, 129, 145, 149, 295, 308, 325), (19...   \n00102686c01fa625aba3b2478f48f415  [(160,), (160,), (198,), (95, 115, 127, 142, 2...   \n0017af9f7fa8ce9e7558bb924cec324e  [(38, 115, 135, 142, 154, 234, 295, 301, 317),...   \n0055347a114b215f8f469fec9e38c272  [(182,), (160, 204), (106, 115, 127, 142, 149,...   \n\n                                  Height  Width  \nImageId                                          \n00000663ed1ff0c4e0132b9b9ac53f6e    5214   3676  \n000aac3870ea7c59ca0333ffa5327323    4000   6000  \n00102686c01fa625aba3b2478f48f415    1742   1500  \n0017af9f7fa8ce9e7558bb924cec324e    1797    607  \n0055347a114b215f8f469fec9e38c272    1500   1000  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>EncodedPixels</th>\n      <th>ClassId</th>\n      <th>AttributesIds</th>\n      <th>Height</th>\n      <th>Width</th>\n    </tr>\n    <tr>\n      <th>ImageId</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>00000663ed1ff0c4e0132b9b9ac53f6e</th>\n      <td>[6068157 7 6073371 20 6078584 34 6083797 48 60...</td>\n      <td>[6, 0, 28, 31, 32, 32, 31, 29, 4]</td>\n      <td>[(115, 136, 143, 154, 230, 295, 316, 317), (11...</td>\n      <td>5214</td>\n      <td>3676</td>\n    </tr>\n    <tr>\n      <th>000aac3870ea7c59ca0333ffa5327323</th>\n      <td>[8971124 17 8975101 49 8979077 82 8983053 116 ...</td>\n      <td>[10, 33, 3, 31, 31, 33]</td>\n      <td>[(104, 115, 129, 145, 149, 295, 308, 325), (19...</td>\n      <td>4000</td>\n      <td>6000</td>\n    </tr>\n    <tr>\n      <th>00102686c01fa625aba3b2478f48f415</th>\n      <td>[1030495 6 1032230 18 1033965 30 1035701 41 10...</td>\n      <td>[31, 31, 33, 10]</td>\n      <td>[(160,), (160,), (198,), (95, 115, 127, 142, 2...</td>\n      <td>1742</td>\n      <td>1500</td>\n    </tr>\n    <tr>\n      <th>0017af9f7fa8ce9e7558bb924cec324e</th>\n      <td>[169688 9 171477 25 173265 43 175053 60 176841...</td>\n      <td>[6, 1, 31, 31, 33]</td>\n      <td>[(38, 115, 135, 142, 154, 234, 295, 301, 317),...</td>\n      <td>1797</td>\n      <td>607</td>\n    </tr>\n    <tr>\n      <th>0055347a114b215f8f469fec9e38c272</th>\n      <td>[893062 1 894561 4 896061 7 897560 10 899059 1...</td>\n      <td>[33, 31, 10]</td>\n      <td>[(182,), (160, 204), (106, 115, 127, 142, 149,...</td>\n      <td>1500</td>\n      <td>1000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "image_df = segment_df.groupby('ImageId')['EncodedPixels', 'ClassId', 'AttributesIds'].agg(lambda x: list(x))\n",
    "size_df = segment_df.groupby('ImageId')['Height', 'Width'].mean()\n",
    "image_df = image_df.join(size_df, on='ImageId')\n",
    "\n",
    "print(\"Total images: \", len(image_df))\n",
    "image_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'str'>\n"
    }
   ],
   "source": [
    "c = segment_df.iloc[0]['EncodedPixels']\n",
    "# c= c.split(',')\n",
    "print(type(c))\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (IMAGE_SIZE, IMAGE_SIZE), interpolation=cv2.INTER_AREA)  \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionDataset(utils.Dataset):\n",
    "\n",
    "    def __init__(self, df):\n",
    "        super().__init__(self)\n",
    "        \n",
    "        # Add classes\n",
    "        for i, name in enumerate(class_names):\n",
    "            self.add_class(\"fashion\", i+1, name)\n",
    "        \n",
    "        for i, name in enumerate(attr_names):\n",
    "            self.add_attribute(\"fashion\", i, name)\n",
    "        # Add images \n",
    "        for i, row in df.iterrows():\n",
    "            self.add_image(\"fashion\", \n",
    "                           image_id=row.name, \n",
    "                           path=str(DATA_DIR/'train'/row.name), \n",
    "                           labels=row['CategoryId'],\n",
    "                           attributes=row['AttributesIds'],\n",
    "                           annotations=row['EncodedPixels'], \n",
    "                           height=row['Height'], width=row['Width'])\n",
    "\n",
    "    \n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        info = self.image_info[image_id]\n",
    "        return info['path'], [label_names[int(x)] for x in info['labels']]\n",
    "    \n",
    "    def load_image(self, image_id):\n",
    "        return resize_image(self.image_info[image_id]['path'])\n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "        info = self.image_info[image_id]\n",
    "                \n",
    "        mask = np.zeros((IMAGE_SIZE, IMAGE_SIZE, len(info['annotations'])), dtype=np.uint8)\n",
    "        labels = []\n",
    "        \n",
    "        for m, (annotation, label) in enumerate(zip(info['annotations'], info['labels'])):\n",
    "            sub_mask = np.full(info['height']*info['width'], 0, dtype=np.uint8)\n",
    "            annotation = [int(x) for x in annotation.split(' ')]\n",
    "            \n",
    "            for i, start_pixel in enumerate(annotation[::2]):\n",
    "                sub_mask[start_pixel: start_pixel+annotation[2*i+1]] = 1\n",
    "\n",
    "            sub_mask = sub_mask.reshape((info['height'], info['width']), order='F')\n",
    "            sub_mask = cv2.resize(sub_mask, (IMAGE_SIZE, IMAGE_SIZE), interpolation=cv2.INTER_NEAREST)\n",
    "            \n",
    "            mask[:, :, m] = sub_mask\n",
    "            labels.append(int(label)+1)\n",
    "            \n",
    "        return mask, np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}